<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Deep Learning Class Project
  | Georgia Tech | Fall 2018: CS 4803 / 7643</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
  <link href="css/bootstrap.css" rel="stylesheet">
<style>
body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Devanagari Character Recognition</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Jonathan Axmann, Chris Collins, Mihir Shrirang Joshi</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2018 CS 4803 / 7643 Deep Learning: Class Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Georgia Tech</span>
<hr>

<!-- Goal -->
<h2>Abstract</h2>

We attempt to perform image classification on the Devanagari character set. For an input of handwritten Devanagari symbols, our model will output the category label (character name) of each character. We will use a variety of models, including a small fully-connected net with two layers, a simple convolutional net with one convlutional layer, and more complex models, as well as pretrained-models, and compare performance. 
<br><br>
<!-- figure -->
<br><br>
<!-- Main Illustrative Figure --> 
<div style="text-align: center;">
<img style="height: 200px;" alt="" src="images/alexnet.png">
</div>

<br><br>
<!-- Introduction -->
<h2>Introduction / Background / Motivation</h2>
<h4>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</h4>
Initially, our project aimed to solve the problem of classifying Devanagari script. 
<br><br>
Devanagari is an Indic script used in India and Nepal, and our dataset contains 36 characters and 10 digits. It is differentiable from many other written languages by the lack of capitalization and the horizontal bar aligned along the top of the script. 
<br><br>
Our training dataset contains 200 examples of each character, for a total of 92,000 images. Each image consists of 32x32 pixels, and 3 color channels. 


<h4>How is it done today, and what are the limits of current practice?</h4>
Current attempts to classify Devanagari script are outlined here:

<ul>

<li><a>https://towardsdatascience.com/devanagari-script-character-recognition-using-machine-learning-6006b40fa6a9</a>
<br>
This project uses Random Forests (but not deep learning) to classify handwritten Devanagari script. It achieves a maximum accuracy score of 92%. 

<li><a>https://www.hindawi.com/journals/cin/2018/6747098/</a>
<br>
This project compares VGG, ResNet, DenseNet, and others to achieve a maximal accuracy of 98% on the given data set. 

</ul>
We (and both of the aforementioned projects) acquired data from:  <a>https://archive.ics.uci.edu/ml/datasets/Devanagari+Handwritten+Character+Dataset</a>.


<h4>Who cares? If you are successful, what difference will it make?</h4>
The goal of our project is to achieve comparable accuracy to the models used in the paper above, but using a simpler model. Ideally, simply tuning hyperparameters and intelligent design decisions will allow us to achieve high accuracy with a less complex model (or a pre-trained model) to reduce overall training time. Our approach could be useful to apply to character recognition tasks when there are limited resources to train on. The paper above trains each model on 250 epochs to achieve a best-accuracy of 98%. We will limit our maximum training epochs to 25, and see how competitive our accuracy will be. 


<br><br>
<!-- Approach -->
<h2>Approach</h2>
<h4>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</h4>
We tried a variety of models, including a simple two-layer fully-connected net, a simple convolutional net, a modified variant of VGG11 (using different input sizes and additional max-pooling layers), a pre-trained AlexNet, a fully-trained VGG11, and more. Our approach was to first use simple models to make sure the data was formatted correctly and that our approach would work at all, as a sanity check. 


<h4>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</h4>
One problem that we faced was having to change the input sizes to some of the pre-trained networks. VGG11, for instance, takes a 224x224x3 image as input, and our images are only 32x32. It's possible to work around this problem by simply resizing the images for the pre-trained models. For fully-trained VGG11, however, it is not feasible to use 224x224x3 images since the memory usage is extremely high. For this reason, we created a VGG11-variant that accepted 32x32x3 input images and still achieved high accuracy. 


<br><br>
<!-- Results -->
<h2>Experiments and Results</h2>
<h4>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</h4>

We measured success by measuring how many of the test set images were correctly categorized into their respective bin out of all the bins. Random accuracy would be about 2.2% accuracy. There were 13800 total images in the test set and our best model correctly categorized 13506 of them with it's first guess. We chose not to use top5 accuracy because it does not make sense to allow to model to guess multiple times on character recognition - it's important to be correct on the first try. Regardless, accuracy is already so high top5 accuracy is likely 100% or close to it. 
<br>
<br>
Our goal was to match the performance of the deep learning paper that achieved 98% accuracy, but do so with a simpler model. We feel that we have achieved this goal. The first paper listed above uses only Random Forests and achieves 92% accuracy, which is fairly easy to beat with pre-trained AlexNet (achieving 96% accuracy). The deep learning model in the paper above uses several models, notably ResNet, DenseNet, and others. These are all trained over at least 250 epochs. Our goal was to use at most a tenth the amount of epochs along with a simpler model to prove that in cases where compute resources are perhaps more limited, it is still feasible to train a deep net to perform a complex classification task. The key to our approach was to modify a variant of VGG11 to take smaller dimensional input rather than the default of 224x224x3 and instead use 32x32x3. This allows the model to train significantly faster (and use far less memory resources), while maintaining similar accuracy in this case. 

<br><br>

<!-- Main Results Figure --> 
<!-- <div style="text-align: center;"> -->

<h3>FC Layer with Softmax Classifier (70% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/softmax_lossvstrain.png" width="49%"/>
<img src="images/softmax_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>

<!-- <p style="font-size: 14px">Filter visualization for softmax classifier</p> -->
<div  style="width:35%">
<img src="images/softmax_filt.png" />
</div>



<h3>Two Layer FC (46% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/twolayernn_lossvstrain.png" width="49%"/>
<img src="images/twolayernn_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>

<div  style="width:35%">
<img src="images/twolayernn32d.png" />
<!-- <p style="font-size: 14px">Filter visualization for two layer FC</p>
 -->
 </div>



<h3>Single Conv Layer with 224x224x3 Input (80% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/convnet224d_80_lossvstrain.png" width="49%"/>
<img src="images/convnet224d_80_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>


<h3>Single Conv Layer with 32x32x3 Input (88% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/convnet32d_88_lossvstrain.png" width="49%"/>
<img src="images/convnet32d_88_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>

<div  style="width:35%">
<img src="images/convnet32d.png" />
<!-- <p style="font-size: 14px">Filter visualization for simple convolutional net with 32x32x3 input</p>
 -->
 </div>


<h3>Pre-Trained AlexNet (last layer tuned - 10 epochs) (97% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/alexnet_lossvstrain.png" width="49%"/>
<img src="images/alexnet_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>



<h3>Pre-Trained AlexNet (last two layers tuned - 10 epochs) (98.2% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/alex2_982_lossvstrain.png" width="49%"/>
<img src="images/alex2_982_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>



<h3>Fully-Trained VGG11 Variant with 32x32x3 Input (10 epochs) (97% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/vgg11_10ep_var_lossvstrain.png" width="49%"/>
<img src="images/vgg11_10ep_var_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>

<div style="width:35%">
<img src="images/vgg11_10ep_var.png" />
<!-- <p style="font-size: 14px">Filter visualizations for variant of vgg11 with 32x32x3 input</p> -->

</div>



<h3>Fully-Trained VGG11 Variant with 32x32x3 Input (15 epochs) (97.8% accuracy)</h3>

<table border=1>
<tr>
<td>
<img src="images/vgg11_15ep_var_lossvstrain.png" width="49%"/>
<img src="images/vgg11_15ep_var_valaccuracy.png"  width="49%"/>
</td>
</tr>
</table>

</div>
<br><br>
  
  <hr>
  <footer> 
  <p>Â© Jonathan Axmann, Chris Collins, Mihir Shrirang Joshi</p>
  </footer>
</div>
<!-- </div> -->

<br><br>

</body></html>
